#!/usr/bin/env python3
"""
Test Excel Catalog Processing
Tests both scenarios: with and without web scraper
"""

import os
import sys
import traceback
from pathlib import Path

# Add src to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))


def test_without_web_scraper(excel_file):
    """Test catalog processing WITHOUT web scraper"""
    print("üî• TESTING WITHOUT WEB SCRAPER")
    print("=" * 60)

    try:
        from core.catalog_processor import CatalogProcessor
        from utils.config import ConfigManager

        config = ConfigManager()
        processor = CatalogProcessor(config)

        print(f"üìÅ Processing Excel file: {excel_file}")

        # Process the Excel file
        products = processor.process_catalog_file(excel_file, 'indian_real_estate')

        print(f"‚úÖ Successfully processed {len(products)} products")

        if not products:
            print("‚ö†Ô∏è  No products found - check Excel file format")
            return []

        # Show statistics
        stats = processor.get_catalog_stats(products)
        print(f"\nüìä Catalog Statistics:")
        print(f"   ‚Ä¢ Total products: {stats.get('total_products', 0)}")
        print(f"   ‚Ä¢ Has descriptions: {stats.get('has_description', 0)}")
        print(f"   ‚Ä¢ Has prices: {stats.get('has_price', 0)}")
        print(f"   ‚Ä¢ Has categories: {stats.get('has_category', 0)}")
        print(f"   ‚Ä¢ Average title length: {stats.get('avg_title_length', 0):.1f} chars")

        # Price statistics
        if 'price_stats' in stats and stats['price_stats']:
            price_stats = stats['price_stats']
            print(f"   ‚Ä¢ Price range: {price_stats.get('min', 0):,.0f} - {price_stats.get('max', 0):,.0f}")
            print(f"   ‚Ä¢ Average price: {price_stats.get('avg', 0):,.0f}")

        # Show field mappings used
        field_mappings = processor._get_field_mappings('indian_real_estate')
        print(f"\nüó∫Ô∏è  Field Mappings Available: {len(field_mappings)} types")

        # Show first few products
        print(f"\nüì¶ Sample Products:")
        for i, product in enumerate(products[:5], 1):
            print(f"   {i}. ID: {product.id}")
            print(f"      Title: {product.title[:60]}...")
            if product.price:
                print(f"      Price: {product.currency} {product.price:,.0f}")
            if product.category:
                print(f"      Category: {product.category}")

            # Show key attributes
            if product.attributes:
                key_attrs = {}
                for key in ['city', 'num_beds', 'num_baths', 'size_sqft', 'agent_name']:
                    if key in product.attributes:
                        key_attrs[key] = product.attributes[key]
                if key_attrs:
                    print(f"      Attributes: {key_attrs}")
            print()

        return products

    except Exception as e:
        print(f"‚ùå Error in processing: {e}")
        traceback.print_exc()
        return []


def test_with_web_scraper(excel_file, products):
    """Test catalog processing WITH web scraper"""
    print("\nüï∑Ô∏è  TESTING WITH WEB SCRAPER")
    print("=" * 60)

    if not products:
        print("‚ö†Ô∏è  No products to enhance - skipping web scraper test")
        return

    try:
        from core.web_scraper import WebScraper
        from utils.config import ConfigManager

        config = ConfigManager()
        scraper = WebScraper(config)

        print(f"üîç Testing web scraper on first 3 products...")
        print(f"üí° Note: This may take 10-30 seconds per product")

        enhanced_count = 0

        for i, product in enumerate(products[:3], 1):
            print(f"\nüèòÔ∏è  Enhancing Product {i}: {product.title[:50]}...")

            try:
                # Enhance with web data
                web_info = scraper.enhance_product_with_web_data(product)

                print(f"   üìä Results:")
                print(f"      ‚Ä¢ Search results: {len(web_info.search_results)}")
                print(f"      ‚Ä¢ Pages scraped: {len(web_info.scraped_content)}")
                print(f"      ‚Ä¢ Specifications: {len(web_info.specifications)}")
                print(f"      ‚Ä¢ Reviews found: {len(web_info.reviews)}")

                # Show sample search results
                if web_info.search_results:
                    print(f"   üîç Sample search results:")
                    for j, result in enumerate(web_info.search_results[:2], 1):
                        title = result.get('title', 'No title')[:40]
                        source = result.get('source', 'unknown')
                        print(f"      {j}. {title}... (from {source})")

                # Show specifications if found
                if web_info.specifications:
                    print(f"   üìã Sample specifications:")
                    for url, specs in list(web_info.specifications.items())[:1]:
                        for key, value in list(specs.items())[:3]:
                            print(f"      ‚Ä¢ {key}: {value[:30]}...")

                enhanced_count += 1

            except Exception as e:
                print(f"   ‚ö†Ô∏è  Web enhancement failed: {e}")
                continue

        print(f"\n‚úÖ Successfully enhanced {enhanced_count} out of 3 products")

        if enhanced_count == 0:
            print("üí° Web scraping notes:")
            print("   ‚Ä¢ May need Google API key for better results")
            print("   ‚Ä¢ DuckDuckGo fallback has limited results")
            print("   ‚Ä¢ Network connectivity required")
            print("   ‚Ä¢ Some sites may block automated requests")

    except Exception as e:
        print(f"‚ùå Error in web scraping: {e}")
        traceback.print_exc()


def analyze_excel_structure(excel_file):
    """Analyze Excel file structure"""
    print("üîç ANALYZING EXCEL STRUCTURE")
    print("=" * 60)

    try:
        import pandas as pd

        # Read just the first few rows
        df = pd.read_excel(excel_file, nrows=5)

        print(f"üìã Excel File Analysis:")
        print(f"   ‚Ä¢ Columns found: {len(df.columns)}")
        print(f"   ‚Ä¢ Sample rows: {len(df)}")

        print(f"\nüìù Column Names:")
        for i, col in enumerate(df.columns, 1):
            print(f"   {i}. {col}")

        print(f"\nüìä Sample Data (first 2 rows):")
        for idx, row in df.head(2).iterrows():
            print(f"   Row {idx + 1}:")
            for col in df.columns[:5]:  # Show first 5 columns
                value = str(row[col])[:30] if pd.notna(row[col]) else "None"
                print(f"      {col}: {value}")
            print()

        return True

    except Exception as e:
        print(f"‚ùå Error analyzing Excel: {e}")
        return False


def main():
    """Main test function"""
    print("üìä EXCEL CATALOG TESTING")
    print("üß™ Testing Real Estate Processing")
    print("=" * 70)

    excel_file = "/Users/amans/Desktop/catalog-auto-tagger/feeds/Book3.xlsx"

    # Check if file exists
    if not Path(excel_file).exists():
        print(f"‚ùå Excel file not found: {excel_file}")
        print("üí° Please ensure the file exists and path is correct")
        return False

    print(f"üìÅ Target file: {excel_file}")
    print(f"üìè File size: {Path(excel_file).stat().st_size / 1024:.1f} KB")

    try:
        # Step 1: Analyze structure
        if not analyze_excel_structure(excel_file):
            return False

        # Step 2: Test without web scraper
        products = test_without_web_scraper(excel_file)

        # Step 3: Test with web scraper (if products found)
        test_with_web_scraper(excel_file, products)

        # Summary
        print(f"\nüéØ TESTING SUMMARY")
        print("=" * 50)
        print(f"‚úÖ Excel structure analysis: Complete")
        print(f"‚úÖ Catalog processing: {'Success' if products else 'No products found'}")
        print(f"‚úÖ Web scraper test: {'Complete' if products else 'Skipped'}")

        print(f"\nüìñ What was tested:")
        print(f"   ‚Ä¢ Excel file reading and parsing")
        print(f"   ‚Ä¢ Meta real estate field mapping")
        print(f"   ‚Ä¢ Indian market price parsing (‚Çπ, lakh, crore)")
        print(f"   ‚Ä¢ Product data extraction and validation")
        if products:
            print(f"   ‚Ä¢ Web search and content scraping")
            print(f"   ‚Ä¢ Specification and review extraction")

        print(f"\nüí° Next Steps:")
        print(f"   1. Review the field mappings for your data")
        print(f"   2. Adjust column names if needed")
        print(f"   3. Configure web scraper with API keys for better results")
        print(f"   4. Process your full catalog when ready")

        return True

    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
